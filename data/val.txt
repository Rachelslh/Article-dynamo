Introduction To Neural Networks - Part 2
Game Changers: MLPs & Backpropagation

In the previous article, we  reviewed the earlier models that helped initiate what Artificial Neural Networks (ANNs) represent today. Check it out if you haven't read it yet. For those who did, you certainly know that non-linearity was still a problem to be overcome. Fortunately, this article tells you all about it, so stick around.
Introduction To Neural Networks - Part 1
Comprehension of The Earliest Models: Perceptron & Adalinetowardsdatascience.com
Multi-layer Perceptron (MLP)…Vanilla NNs
This introduces the so-called "Layers". Technically, a layer is a container that receives data, applies a linear/nonlinear transform function to it, and shifts it to the next layer.
There will be "Layers" everywhere, just bear with me…
Strictly speaking, a layer holds a set of perceptrons (excluding the input layer), each one is connected to all the perceptrons present in the next layer to allow data flow, where the data is the set of outputs of all neurons present in that particular layer. 
There is no need for a model description here for as the name MLP speaks for itself, it is indeed a neural network composed of at least three layers where:
First layer: The input layer. Provides the features x₁…xₙ per sample x.
Last layer:  The output layer.
In-between layers: Hidden layers, the simplest architecture contains one hidden layer, the model will then be called a Shallow Neural Network. Otherwise, it's a Deep NN.

Although you might think that a perceptron in an MLP network is the same earlier model discussed in the last article. Well, it is not. The only difference here lies in the choice of the activation function which is no longer linked to a threshold function, but even nonlinear ones are now allowed, which allows us to finally move from the classification of linearly separable data to non-separable ones. Note that neurons in the same layer must share the same activation function;
Activation Functions, Mathematical Gates
A crucial step in determining the MLP model that will best suit your data and hypothesis is the choice of the activation function because its output will determine for the neuron whether it should fire or not!
Activation functions differ depending on the type:
Binary step functions: Threshold-based functions as seen before in Perceptron and Adaline models. Limited to the classification of two categorical outputs only.
Linear functions: The output is proportional to the input due to the shape of linear functions that is Y = aX+b where a and b are constants, X and Y are the input and output respectively. Even if the use of this type of function has shown a significant improvement in Adaline, it remains less efficient and even unreliable compared to nonlinear functions in MLP models.
Nonlinear functions: Widely used in MLPs and deep learning models, allowing variation in output (not proportional to input). This type of function offers several options, the most used are Sigmoid and Rectified Linear Unit (Relu). Take a look at this for more details on nonlinear functions.

AI requires the use of nonlinear functions, why?
Suppose you have a minimalist MLP architecture made up of 3 layers including input and output layers. The recurring use of the linear type as an activation function for the whole model causes the output layer to be a linear function of the first layer. Regardless of the number of hidden layers, the activation function will no longer have an impact on the final results.
As you walk through the model, layer by layer, you will notice that the numbers resulting from a linear function will continue to increase, making the calculus much more complex. This case is irreproducible when using a nonlinear activation function because it will reduce the value of the outcome to a probability before moving it to the next layer.

Forward pass
Perceptron, Adaline, MLPs…These are all feedforward models as explained in the previous article. Therefore, the forward pass of an MLP is quite similar to what you have seen before but more of a multiplex algorithm.

As shown above, each perceptron is related to all the nodes present in the preceding and the following layer, this is why the weight notation is: 
As for the learning technique, Perceptron uses Rosenblatt's learning rule and Adaline uses the Delta rule, Whereas MLPs adopt a supervised learning algorithm called "Backpropagation".

Back-propagation
A huge breakthrough in the history of neural networks, first introduced in 1986 by Seppo Linnainmaa, its name is representative of its working principle, which is the backward propagation of errors through layers.
This valued algorithm   uses heavy differential calculus of the cost/error function with respect to the neural network's weights. This can be seen as a generalization of the delta rule to multi-layer feed-forward neural networks.

Let’s say, a group of friends went out to a restaurant and they all agreed on splitting the bill equally. The restaurant offers different kinds of meals when it comes to pricing: cheap/affordable ones as well as expensive ones. It is known that the more food costs the more it tastes better, but the satisfaction is not really worth the money you end up spending on the fancy meal when you’re on your own, this is where the dilemma comes into effect! Each one of them would like to take advantage of the situation and order the most expensive meal which justifies why this situation is also called The Unscrupulous Dinner.
Theoretical Perspective
The classical dinner’s dilemma model is:
Non-cooperative: It’s all about self-serving first as explained above, each one would rather benefit from this dinner and order the priciest dish thinking that others will certainly help defray the cost by ordering the cheapest dish.
Non-zero-sum: Knowing each player’s chosen strategy, the sum of all payoffs is not null because someone’s gain does not necessarily determine someone else’s loss, you’ll get that once you reach the payoff matrix part in this article.
Symmetric game: The payoff of each player depends on their strategy choice (Cheap or Expensive) but not on their identity.
Now let’s introduce some variables to represent our thoughts!
α: The satisfaction of eating the expensive meal
β: The satisfaction of eating the cheap meal
γ: The cost of the expensive meal
δ: The cost of the cheap meal
It is noticeable that:

The bill will be divided evenly among the entire group. Consequently, ordering an expensive meal seems more joyful than just regular food. Giving N the number of participants, we can translate the player’s ordering preferences this way:

The equations (1) and (2) form the needed rules to come up with a good configuration of the tuple (α, β, γ, δ).
For the utility part, we can either use probabilities which have been demonstrated in Research Gate’s article or we can use a much simpler method that is to define x as the cost of the other players’ meals also described here.
Implementation & Simulation
The game is symmetric and so calculating one player’s utility is sufficient to generate the entire payoff matrix. I have used the binary system because there are only 2 strategies to choose from, where:
0: Cheap
1: Expensive
For N = 3, this is what we get in binary :

We can now generate a table that assigns a payoff to each combination with the following configuration as an example:
(α, β, γ, δ) = (10, 7, 9, 1)

To fill the payoff matrix correctly, we need to look for a symmetrical relation by observing the different binary combinations, it might be tricky to find so feel free to check my source code.
The payoff matrix should look like this where each element is a tuple of the form (player 1’s payoff, …, player N‘s payoff)

Payoff Matrix For N = 3
The code contains other algorithms for the following notions:
Dominant Strategy: The ‘Expensive’ Strategy.
Iterated Elimination of Strictly Dominated Strategies (IESDS): The case where everyone chooses the ‘Expensive’ strategy.
Nash Equilibrium: Same result as the IESDS
Pareto Optimal Point: The case where everyone chooses the ‘Cheap’ strategy.
Security Level of both Strategies and players.
Note: I have used Python and Dash by Plotly.