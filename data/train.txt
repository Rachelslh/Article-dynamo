Introduction To Neural Networks — Part 1
Comprehension of The Earliest Models: Perceptron & Adaline

What is the purpose of AI? Undoubtedly, it is to satisfy humankind’s hunger for creating human-like machines using different concepts, models, and techniques that are still getting improvements because the more you grow, the more your needs grow with you. The thought of simulating the human neuron or in other words creating an artificial one came in 1943 when Warren McCulloch and Walter Pitts modeled a simple neuron with electrical circuits; of-course this remains a very small glimpse on the history of artificial neural networks (ANN). If you want to know more then have a look at this.
Motivation
Over the years, AI knew very different models such as:
Linear Regression with one variable/multiple variables
Logistic Regression (classification) for one class/multi-class (one-vs-all technique)
The common factor between all these mentioned models is their linearity, they can only be applied on linearly-separable data which was in fact one of the reasons that led to the creation of the neural network model, even if the earliest models were also linear classifiers (linear separators). Essentially it was and still is an algorithm that tries to mimic the brain which is by far the most powerful learning machine we know.

Image by author
So, before embarking on a journey of theories, let’s first understand what a feedforward neural network is;
Feedforward NN: is an ANN wherein connections between the nodes (defined below) do not form a cycle. Different from its descendant recurrent neural networks (RNN)¹ (to be seen in future articles). The discussed models in this article are of type feedforward neural networks.
Perceptron, The Root
A Single-Layer Neural Network, a Linear Threshold Unit (LTU)/Gate (LTG), or even one neuron…call it whatever you want. The human neuron is a cell that receives electrical signals as inputs from other cells and outputs a signal based on that. This neuron does the exact same job using numerical values instead, this is when Math becomes your best friend!
If you’re familiar with previously mentioned linear classifiers then you certainly know what a hypothesis function is, and what a decision boundary stands for. If not, let me introduce that quickly:
Hypothesis: The function that best-describes the target known as Y.
Decision Boundary: The best-fitting line, able to separate the data according to different class labels. The objective of each classifier is to find the decision boundary. Note that in binary classification, it’s a battle between True and False values i.e. ones and zeros.
The model description is as follows:
Inputs (X): Single sample x, a vector of features (attributes) x₁…xₙ
Weights (W): Known as Θ (Theta) occasionally, a vector of parameters used to form connections between the different nodes, each weight is associated with an input xᵢ and shows how much influence it will have on the output.
Bias (b): The node x₀, with a constant value of 1. Guarantees an activation of the neuron even when all inputs are null.
Net Input (net): A dot product of weights and inputs, most of the time called z.

Activation Function: Threshold function g(z), which makes the Perceptron a binary classifier (2 class label functions). The threshold is denoted as θ.

In order to simplify g(z), we bring θ to the left side and consider it as the parameter w₀ associated with the bias, g(z) and z become:


Output (Hypothesis): The result of the activation function, referred to as o(x) which is also the result of the hypothesis h(x).

2D graphical illustration of the decision boundary — Image by author
Perceptron Training Rule…Thank You Rosenblatt
Let’s say the weights have been randomly initialized with small numbers, first sample x is running through the Perceptron model, the final output indicates a wrong class, what to do now? Is this AI?
Luckily no, the model will learn at each iteration by improving w values so it can perform better using the following training/learning rule introduced by Rosenblatt in 1959:

Learning Rate: A hyper-parameter² that controls how much to change the model in response to the estimated error each time the model weights are updated³. Traditional values are 0.1 or 0.01, always in the range of 0 and 1.

Schematic of a Perceptron Classifier — Image by mlxtend
Now, we are ready for the algorithm:
- Random initialization of the weights, with small numbers
- For each epoch:
  - For each training sample xϵX:
     .  Calculate output value o(x)
     .  For i from 1..n:
           wᵢ = wᵢ + ∆wᵢ
This little monster is actually capable of representing various functions like And, Or, Nand, Nor, m-of-n…and the list goes on. But there are also lots of cases where it would fail such as XOR function because this one is not linearly-separable so the model wouldn’t be able to find the right decision boundary for it and would stay inside the loop infinitely using the same weights UNLESS you combine multiple Perceptrons, in other words, form a network of Perceptrons (can’t be generalized for all linearly non-separable data):

If we assign a Perceptron to each logical operation shown above, we would end up training a complex one using three already trained models (And, Or and Not). For more details, check this out.
The Perceptron model remains a computational version of the McCulloch-Pitts neuron mentioned above, it does certainly not satisfy the requirements of modern AI. That is why Adaline showed up!! (Not The Movie)
The Era Of ADAptive LInear NEurons (Adaline)
Same Structure As A Perceptron, Different Engine!
Developed by Widrow and Hoff in 1960, considered as the ground-work of many sophisticated machine learning algorithms out there like logistic regression and Support Vector Machines (SVMs). Also a binary classifier and a linear separator.

Activation Function: Linear function (identity function) of the net input z, denoted as o(x).
The use of a threshold function g(z) is still needed.
Unlike the Perceptron model, Adaline uses the continuous outcome z to learn the model weights. It’s much more effective than just utilizing predicted class labels.

Schematic of Adaline Classifier — Image by mlxtend
Adaline is similar to applying linear regression, the only difference is the use of a threshold function to convert the final output to a categorical one. The model uses the so-called cost function J(w) to estimate its error. The logic behind this is that the best-fitting line is defined as the line that minimizes the sum of squared errors (SSE). An error is perceived as the vertical deviation (residual) of a data point (one sample) from the fitted line (decision boundary). Some details might seem a bit blurry at this stage but will be further explained if you keep reading.

Where X is the set of training samples, y(x) the target of a training sample x and o(x) is the continuous outcome (net input) z.
The cost function is the sum of squared errors (convex), divided by 2 to facilitate the derivation process.

2D graphical illustration of the vertical deviations in a linear regression model — Image by author
Optimization…Here We Come
We talked about the cost function and happen to mention the term minimization…Because the main reason error functions exist is to minimize the error hence their name!
We recognize two main approaches to do that:
Normal equations (closed-form solution)
Standard/Stochastic gradient descent
In this article, we‘ll be covering standard and stochastic gradient descent techniques. The procedure is straight forward, we calculate the value of the cost function at each iteration, make a step towards the global minimum of the function by looking for the tangent (partial derivative) at a specific point (w, J(w)), and update the weights using The Delta Rule. The loop stops when the global minimum (or local minimum) is reached (null derivative).
Hmm..Delta Rule…What‘s up with that?
Remember how in a Perceptron, the weights are updated thanks to Rosenblatt’s learning rule, Same premise in Adaline, but with a different formula.

The minus sign indicates that we are taking a step in the opposite direction of the cost gradient as shown in the figure.
Gradient Descent
Now that we have reached the derivation part, you can skip it if you’re only interested in the resulting expression. Otherwise, the derivation process is as follows:

The partial derivative of the cost function J(w) w.r.t each weight wᵢ (i ϵ 1..n) is:


Gradient Descent — Image by mlxtend
Standard Gradient Descent (GD)
Known as Batch-GD mode for updating weights after processing the entire dataset, thus the step length is larger. However, it does not guarantee to find the global minimum and might converge easily to a local one if the cost function is non-convex.
- Random initialization of the weights, with small numbers
- For each epoch :
  .  Calculate output value o(x)for each training sample xϵX
  .  Calculate cost gradient ∇J
  .  For i from 1..n :
        wᵢ = wᵢ + ∆wᵢ
Stochastic Gradient Descent (SGD)
Also referred to as iterative/on-line mode for calculating the cost gradient and updating weights after each training sample which makes it more likely to find the global minimum even if the cost function is non-convex. Consequently, it takes more time to converge (reach the cost minimum).
- Random initialization of the weights, with small numbers
- For each epoch :
  - For each batch training sample xϵX :
     .  Calculate the output value o(x)
     .  Calculate cost gradient ∇J
     .  For i from 1..n :
           wᵢ = wᵢ + ∆wᵢ
Ways To Make GD Algorithms Converge Faster
In large scale machine learning systems, it’s common to use Mini-Batches that is a compromise between GD and SGD. Produces a smoother convergence than SGD.
When features differ by orders of magnitude, feature scaling provides numerical stability that is more likely to make GD converge faster. Various methods exist such as Min-Max Normalization and Standardization.
Inspecting Learning Rate Values
This section shows the importance of plotting learning curves when it comes to gradient descent algorithms. Sometimes, when GD does not converge well or maybe not at all, the problem might be in your chosen learning rate value.
Too Large: The algorithm might overshoot the minimum and diverge as shown in the left graph.
Too Small: might require too many epochs to converge (small steps) and can lead to a local minimum rather than the global.

GD behavior with a large vs small learning rate
Equation Of The Decision Boundary
The equation of a separating line when you have a 2-dimensional input vector is of type:

Where a is called the slope and b the y-intercept.
First, set the net input z to 0;

If you replace x₁, x₂, w₁, w₂ and w₀ by x, y, A, B, and C respectively, it would look like a standard equation of a line.

The next step is to look for the x-intercept and the y-intercept, that way you get two points of the line so you can finally find the slope that is a.
Summary
The two initiating models of neural networks are :
Perceptron
Adaptive Linear Neuron (Adaline)
Similarities:
Linear classifier
Binary classier
The use of a threshold function
Differences:
The perceptron model uses its predicted class labels (categorical outputs) and the Perceptron learning rule to learn its coefficients (weights).
On the other hand, Adaline uses its continuous outcome and the delta rule for more accuracy.
If the GD algorithm is still stuck after several epochs, then you might need to consider tweaking your learning rate value for better performance.

So here we are diving into the world of data mining this time, let’s begin with a small but informative definition;
What is data mining ?!
It’s technically a profound dive into datasets searching for some correlations, rules, anomaly detection and the list goes on. It’s a way to do some simple but effective machine learning instead of doing it the hard way like using regular neural networks or the ultimate complex version that is convolutions and recurrent neural networks (we will definitely go through that thoroughly in future articles).
Data mining algorithms vary from one to another, each one has its own pros and cons, i will not go through that in this article but the first one you should focus on must be the classical Apriori Algorithm as it is the opening gate to the data mining world.
But before going any further, there’s some special data mining vocabulary that we need to get familiar with :
k-Itemsets: an itemset is just a set of items, the k refers to its order/length which means the number of items contained in the itemset.
Transaction: it is a captured data, can refer to purchased items in a store. Note that the Apriori algorithm operates on datasets containing thousands or even millions of transactions.
Association rule: an antecedent → consequent relationship between two itemsets :

Implies the presence of the itemset Y (consequent) in the considered transaction given the itemset X (antecedent).
Support: represents the popularity/frequency of an itemset, calculated this way :

Confidence ( X → Y ): shows how much a rule is confident/true, in other words, the likelihood of having the consequent itemset in a transaction, calculated this way :

A rule is called a strong rule if its confidence is equal to 1.
Lift ( X → Y ): A measure of performance, indicates the quality of an association rule :

MinSup: a user-specified variable that stands for the minimum support threshold for itemsets.
MinConf: a user-specified variable that stands for the minimum confidence threshold for rules.
Frequent itemset: whose support is equal or higher than the chosen minsup.
Infrequent itemset: whose support is less than the chosen minsup.
So…how does Apriori work?
Starting with a historical glimpse, the algorithm was first proposed by the computer scientists Agrawal and Srikant in 1994, it proceeds this way :
Generates possible combinations of k-itemsets (starts with k=1)
Calculates support according to each itemset
Eliminates infrequent itemsets
Increments k and repeats the process
Now, how to generate those itemsets ?!!
For itemsets of length k=2, it is required to consider every possible combination of two items (no permutation is needed). For k > 2, two conditions must be satisfied first :
The combined itemset must be formed of two frequent ones of length k-1, let’s call’em subsets.
Both subsets must have the same prefix of length k-2
If you think about it, these steps will just extend the previously found frequent itemsets, this is called the ‘bottom-up’ approach. It also proves that the Apriori algorithm respects the monotone property :
All subsets of a frequent itemset must also be frequent.
As well as the anti-monotone property :
All super-sets of an infrequent itemset must also be infrequent.
Okay, but wait a minute, this seems infinite !!
No, luckily it is not infinite, the algorithm stops at a certain order k if :
All the generated itemsets of length k are infrequent
No found prefix of length k-2 in common which makes it impossible to generate new itemsets of length k
Sure…it’s not rocket science! but how about an example to make this clearer?
Here’s a small transaction table in binary format, the value of an item is 1 if it’s present in the considered transaction, otherwise it’s 0.

Great…It’s time for some association rule mining!

Once you reach this part, all there’s left to do is to take one frequent k-itemset at a time and generate all its possible rules using binary partitioning.
If the 3-itemset {Almonds-Sugar-Milk} from the previous example were a frequent itemset, then the generated rules would look like :

An overview of my Apriori simulation !! Using Python
Dataset
Of format csv (Comma-separated values), containing 7501 transactions of purchased items in a supermarket. Restructuring the dataset with the transaction encoder class from mlxtend library made the use and manipulation much easier. The resulting structure is occupying an area of ​​871.8 KB with 119 columns indexed respectively by food name from “Almonds” to “Zucchini”.
Here’s an overview of the transaction table before and after :

Implementing the algorithm
I will not be posting any code fragments as it was a straight forward approach, the procedure is recursive, calls the responsible functions for the itemsets generation, support calculation, elimination, and association rule mining in the mentioned order.
The execution took 177 seconds which seemed optimized and efficient thanks to Pandas and NumPy’s ability to perform quick element-wise operations. All found association rules were saved in an Html file for later use.
Now, how about a tour in the supermarket? Using Dash by Plotly
Finally, I got to use the previously saved rules to suggest food items based on what my basket contains. Here’s a quick preview :

Feel free to check my source code here.